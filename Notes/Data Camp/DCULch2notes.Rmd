---
title: "DCULch2notes"
author: "Corey Pang"
output: html_document
---
### Hierarchical Clustering
* Number of clusters is not known ahead of time
    * Different from __kmeans__ because you do not specifiy the number of clusters beforehand
    1. Bottom-up
        * Closest two observations/clusters are joined together into a cluster
        * This process repeats until there is only one cluster left
        * _hclust(d = distance)_ where distance can be computed through _dist(x)_
    2. Top-down
    
#### Selecting the Number of Clusters 

```{r sample}
x=rbind(3,3.1,4.1,4,3.5)
y=rbind(3,3.5,3.9,3.5,2.4)
data = cbind(x,y)
plot(3,3, col='red')
points(3.1,3.5, col='green')
points(4.1,3.9, col='blue')
points(4,3.5, col='pink')
points(3.5,2.4, col='black')
hclust.dist <- dist(data)
hclust.out = hclust(hclust.dist)
plot(hclust.out)
abline(h=1, col='red')
cutree(hclust.out, h=1)
cutree(hclust.out, k=2)
```
1. _plot(hclust.out)_ to get a Dendrogram chart
2. Choose your height, and add line with _abline(h=height, col=red)_
3. Cut tree function will specify which distance you want to pick as the cut off for clusters. _cutree(hclust.out, h=height)_ __OR__ _cutree(hclust.out, k=# of clusters)_

#### Distance between Clusters
1. __Complete:__ Pairwise Similarity between all observations in cluster 1 and 2, and uses the largest of similarities
2. __Single:__ Same as Complete but uses the smallest of similarities
3. __Average:__ Same as Complete and Singles but uses the average of the similarities
4. __Centroid:__ Finds centroid of both clusters 1 and 2, and uses similarity between two centroids
* To choose these: hclust(x, method='')
* Normalize the data when there are different units or if the means and standard deviations are different
    * To check whether normalization is needed: 
        * _colmeans(x)_
        * _apply(x, 2, sd)_
        * _scaled.x <- scale(x)_
        
